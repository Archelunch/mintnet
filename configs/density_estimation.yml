training:
  n_epochs: 500
  batch_size: 32
  log_interval: 1
  snapshot_interval: 10

data:
#  dataset: MNIST
#  lambda_logit: 0.000001
#  image_size: 28
#  channels: 1
#  horizontal_flip: false
  dataset: CIFAR10
  image_size: 32
  lambda_logit: 0.05
  channels: 3
  horizontal_flip: false

model:
## mnist model
#  n_layers: 16
#  latent_size: 32
#  n_subsampling: 2
#  act_norm: true

## cifar10 model
  n_layers: 16
  latent_size: 85
  n_subsampling: 2
  act_norm: false


optim:
  optimizer: Adam
  lr: 0.0003
  beta1: 0.9
  weight_decay: 0
  amsgrad: true
  adam_eps: 0.0001
