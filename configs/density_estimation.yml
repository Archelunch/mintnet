training:
  n_epochs: 500
  batch_size: 256
  log_interval: 100
  snapshot_interval: 10

data:
  dataset: MNIST
  lambda_logit: 0.000001
  image_size: 28
  channels: 1
#  dataset: CIFAR10
#  image_size: 32
#  lambda_logit: 0.05
#  channels: 3

model:
  layer_size:
    - 1
    - 1
    - 19

  latent_size:
    - 32
    - 32
    - 32

optim:
  optimizer: Adam
  lr: 0.001
  beta1: 0.9
  weight_decay: 0.000001
