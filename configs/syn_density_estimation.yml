training:
  n_epochs: 1000
  batch_size: 10000
  log_interval: 10
  snapshot_interval: 1000
  evaluation: 10000

data:
  toy: 2
  function_index: 2
  channels: 2
  image_size: 1
  lambda_logit: 0.00001

model:
  n_layers: 16
  latent_size: 32
  n_subsampling: 0
  act_norm: false
  batch_norm: false
  rgb_last: false
  zero_init_start: 100

  #for sampling
  n_iters: 20

optim:
  optimizer: Adam
  lr: 0.01
  beta1: 0.9
  weight_decay: 0 #.0001
  amsgrad: true
  adam_eps: 0.0001
